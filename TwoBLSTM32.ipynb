{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"TwoBLSTM32.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KRpch3wzt0l2","colab_type":"code","outputId":"9f1e13c9-d64e-4506-bd60-0c9516cfae89","executionInfo":{"status":"ok","timestamp":1587690267278,"user_tz":240,"elapsed":28355,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KiutzjCvosYR","colab_type":"code","outputId":"f0409f79-8e93-4c9c-f90f-0b7605fcd4cb","executionInfo":{"status":"ok","timestamp":1587690296332,"user_tz":240,"elapsed":1687,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["from torch.utils.data import Dataset\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","import time\n","import numpy as np\n","import pathlib\n","import time\n","import datetime \n","import json\n","import random\n","import sys \n","import os\n","# CHANGE THIS TO NAME OF CURRENT NOTEBOOK!\n","CURR_NOTEBOOK = \"twoblstm32\"\n","\n","print(\"Last run time: \", datetime.datetime.now())\n","print(\"Files in Directory: \" ,os.listdir(\"drive/My Drive/Colab Notebooks\"))\n","print(\"Absolute Path:\", pathlib.Path().absolute())\n","print(\"Current Notebook: \", CURR_NOTEBOOK)\n","\n","path = \"drive/My Drive/Colab Notebooks/\"\n","\n","# Make Models data directory if it doesn't exist\n","if not os.path.exists(path + \"models/\" + CURR_NOTEBOOK):\n","    os.makedirs(path + \"models/\"+ CURR_NOTEBOOK)\n","    print(CURR_NOTEBOOK  + \" directory created at \" + path + CURR_NOTEBOOK)\n","\n","# Check CUDA\n","cuda = torch.cuda.is_available()\n","\n","# Seed Random Number generator for CPU & CUDA\n","torch.manual_seed(1)\n","if cuda:\n","    torch.cuda.manual_seed(1)\n","kwargs = {'num_workers': 2, 'pin_memory': True} if cuda else {}\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Last run time:  2020-04-24 01:04:54.998015\n","Files in Directory:  ['data', 'models', 'PreprocessData-2.ipynb', 'GRU-Rand.ipynb', 'Old', 'TransferLSTM.ipynb', 'AppendData.ipynb', 'cs7643colab.ipynb', 'hyperparams.json', 'Ntb_Att-GRU.ipynb', 'TestLSTM.ipynb', 'Ntb_GRU.ipynb', 'SingleLSTM32.ipynb', 'TwoBLSTMs.ipynb', 'TestLSTM32.ipynb', 'Model Notes.gdoc', 'LSTM-GRU-Rand.ipynb', 'NTB_GRU-LSTM32.ipynb', 'meetingnotes.gdoc', 'NTB_GRU32.ipynb', 'TestBERT.ipynb', 'TestBERT32.ipynb', 'TwoBLSTM32.ipynb']\n","Absolute Path: /content\n","Current Notebook:  twoblstm32\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vIFWew_TosYc","colab_type":"code","outputId":"c57db987-b711-4efe-c74c-66b1e42b2d77","executionInfo":{"status":"ok","timestamp":1587690298826,"user_tz":240,"elapsed":1965,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["MAX_SEQ_LEN = 32\n","FRAC_OF_TRAIN = .70\n","FRAC_OF_VAL = .20\n","FRAC_OF_TEST = .10\n","\n","### Assign path to master file of data\n","words_file_master = path + \"data/32/TED_words_master_32_q.txt\"\n","punct_file_master = path + \"data/32/TED_punc_master_32_q.txt\"\n","\n","print(words_file_master)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["drive/My Drive/Colab Notebooks/data/32/TED_words_master_32_q.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GY5PE7qRosYw","colab_type":"code","outputId":"03eb023d-b05b-4395-ddb5-b366bbf01fec","executionInfo":{"status":"ok","timestamp":1587690305652,"user_tz":240,"elapsed":6385,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["class PunctuationDataset(Dataset):\n","    def __init__(self, split='train'):\n","        self.split = split\n","        self.train_data = []\n","        self.train_labels = []\n","        self.val_data = []\n","        self.val_labels = []\n","        self.test_data = []\n","        self.test_labels = []\n","        self.word_to_ix = {}\n","        self.tag_to_ix = {'_': 0, '.': 1, ',': 2, '?' : 3}\n","        \n","        # w_file = open(words_file, 'r',encoding='utf-8') \n","        # p_file = open(punct_file, 'r',encoding='utf-8')\n","\n","        input_file = open (path + \"models/indices32.json\")\n","        json_array = json.load(input_file)\n","        train_idx = json_array[\"train_idx\"]\n","        val_idx = json_array[\"val_idx\"]\n","        test_idx = json_array[\"test_idx\"]\n","\n","        #####################################################################################\n","        #                                IN PROGRESS                                        #\n","        #####################################################################################\n","        # with open(words_file_master) as f:\n","        #   words_master_total_lines = sum(1 for _ in f)\n","\n","        # with open(punct_file_master) as f:\n","        #   punc_master_total_lines = sum(1 for _ in f)\n","\n","        # print(\"Total Number of Lines: \", words_master_total_lines, \\\n","        #       \"Total Number of Punc Lines: \", punc_master_total_lines)\n","        \n","        # # Get Size of training sets (set fractions above)\n","        # SIZE_OF_TRAIN_WORDS = int(np.floor(FRAC_OF_TRAIN * words_master_total_lines))\n","        # SIZE_OF_VAL_WORDS = int(np.ceil(FRAC_OF_VAL * words_master_total_lines))\n","        # SIZE_OF_TEST_WORDS =  int(words_master_total_lines - SIZE_OF_TRAIN_WORDS - SIZE_OF_VAL_WORDS)\n"," \n","        # print(\"Training set size: \", SIZE_OF_TRAIN_WORDS, \", Validation Set Size: \", SIZE_OF_VAL_WORDS, \\\n","        #       \", Test Set Size: \", SIZE_OF_TEST_WORDS)\n","        \n","        # # Get randomly generated indices\n","        # train_idx = random.sample(range(0, words_master_total_lines), SIZE_OF_TRAIN_WORDS) # Get random sample for training\n","        # remaining_idx = [idx for idx in range(0, words_master_total_lines) if idx not in train_idx] # Get indices not in training set\n","        # val_idx = random.sample(remaining_idx, SIZE_OF_VAL_WORDS) # Get random subset of remaining indices\n","        # test_idx = [idx for idx in remaining_idx if idx not in val_idx] # Get remaining indices for test set\n","        # save_indices(train_idx, val_idx, test_idx)\n","\n","        # Read the lines in the master file\n","        wordslst = open(words_file_master).readlines()\n","        punclst  = open(punct_file_master).readlines()\n","\n","        # Get assign sets from files\n","        words_training_set = [wordslst[i] for i in train_idx]\n","        words_val_set = [wordslst[i] for i in val_idx]\n","        words_test_set = [wordslst[i] for i in test_idx]\n","\n","        punc_training_set = [punclst[i] for i in train_idx]\n","        punc_val_set = [punclst[i] for i in val_idx]\n","        punc_test_set = [punclst[i] for i in test_idx]\n","        # print(templst[int(test_idx)])\n","        #####################################################################################\n","        #                                IN PROGRESS                                        #\n","        #####################################################################################\n","\n","        # TRAINING DATA\n","        # \n","        # Loop through lines in word & punctuation file\n","        # for la, lb in zip(w_file, p_file):\n","        for la, lb in zip(words_training_set, punc_training_set):\n","            words = la.split()\n","            punc_lst = list(lb[:-1])\n","\n","            if len(words) != len(punc_lst) or len(words) != MAX_SEQ_LEN: #TODO: need to fix preprocessing\n","                continue\n","\n","           # Loop through words & save length of word\n","            for i in range(len(words)):\n","                w = words[i]\n","                \n","                # Save word if not in word_to_ix w/ respective index\n","                if w not in self.word_to_ix:\n","                    self.word_to_ix[w] = len(self.word_to_ix)\n","                words[i] = self.word_to_ix[w]\n","            \n","            # Loop through punctuation & assign proper index tag\n","            for i in range(len(punc_lst)):\n","                punc_lst[i] = self.tag_to_ix[punc_lst[i]]\n","\n","            self.train_data.append(words)\n","            self.train_labels.append(punc_lst)\n","            \n","        # w_file.close()\n","        # p_file.close()\n","\n","        self.train_data = torch.from_numpy(np.matrix(self.train_data, dtype='int64'))\n","        self.train_labels = torch.from_numpy(np.matrix(self.train_labels, dtype='int64'))\n","        \n","        #TODO: use val data\n","#         self.val_data = self.train_data\n","#         self.val_labels = self.train_labels\n","        # w_file = open(words_file_dev, 'r',encoding='utf-8') \n","        # p_file = open(punct_file_dev, 'r',encoding='utf-8')\n","        \n","        # TRAINING DATA\n","        # \n","        # Loop through lines in word & punctuation file\n","        # for la, lb in zip(w_file, p_file):\n","        for la, lb in zip(words_val_set, punc_val_set):\n","            words = la.split()\n","            punc_lst = list(lb[:-1])\n","\n","            if len(words) != len(punc_lst) or len(words) != MAX_SEQ_LEN: #TODO: need to fix preprocessing\n","                continue\n","\n","            for i in range(len(words)):\n","                w = words[i]\n","                if w not in self.word_to_ix:\n","                    self.word_to_ix[w] = len(self.word_to_ix)\n","                words[i] = self.word_to_ix[w]\n","            \n","            for i in range(len(punc_lst)):\n","                punc_lst[i] = self.tag_to_ix[punc_lst[i]]\n","            \n","            self.val_data.append(words)\n","            self.val_labels.append(punc_lst)\n","            \n","        # w_file.close()\n","        # p_file.close()\n","        self.val_data = torch.from_numpy(np.matrix(self.val_data, dtype='int64'))\n","        self.val_labels = torch.from_numpy(np.matrix(self.val_labels, dtype='int64'))\n","        \n","        #TODO: use test data\n","        for la, lb in zip(words_test_set, punc_test_set):\n","            words = la.split()\n","            punc_lst = list(lb[:-1])\n","\n","            if len(words) != len(punc_lst) or len(words) != MAX_SEQ_LEN: #TODO: need to fix preprocessing\n","                continue\n","\n","            for i in range(len(words)):\n","                w = words[i]\n","                if w not in self.word_to_ix:\n","                    self.word_to_ix[w] = len(self.word_to_ix)\n","                words[i] = self.word_to_ix[w]\n","            \n","            for i in range(len(punc_lst)):\n","                punc_lst[i] = self.tag_to_ix[punc_lst[i]]\n","            \n","            self.test_data.append(words)\n","            self.test_labels.append(punc_lst)\n","            \n","        # w_file.close()\n","        # p_file.close()\n","        self.test_data = torch.from_numpy(np.matrix(self.test_data, dtype='int64'))\n","        self.test_labels = torch.from_numpy(np.matrix(self.test_labels, dtype='int64'))\n"," \n","    def __len__(self):\n","        if self.split == 'train':\n","            return len(self.train_data)\n","        elif self.split == 'val':\n","            return len(self.val_data)\n","        else:\n","            return len(self.test_data)\n"," \n","    def __getitem__(self,idx):\n","        if self.split == 'train':\n","            return (self.train_data[idx], self.train_labels[idx])\n","        elif self.split == 'val':\n","            return (self.val_data[idx], self.val_labels[idx])\n","        else:\n","            return (self.test_data[idx], self.test_labels[idx])\n","        \n","    def set_split(self, split):\n","        self.split = split\n"," \n","punc_dataset = PunctuationDataset()\n","print(\"Size of Training Data: \", len(punc_dataset.train_data))\n","print(\"Size of Validation Data: \", len(punc_dataset.val_data))\n","print(\"Size of Test Data: \", len(punc_dataset.test_data))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Size of Training Data:  49476\n","Size of Validation Data:  14136\n","Size of Test Data:  7064\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m5RJJsZVosY8","colab_type":"code","outputId":"534c1298-a168-4696-e3e2-9bca7a2273f4","executionInfo":{"status":"ok","timestamp":1587690307658,"user_tz":240,"elapsed":1996,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Hyperparameters\n","# EMBEDDING_DIM = 64\n","# HIDDEN_DIM = 256\n","# LEARNING_RATE = 0.1\n","# WEIGHT_DECAY = 0\n","# EPOCHS = 5\n","# MOMENTUM = 0\n","input_file = open (path + \"/hyperparams.json\")\n","json_array = json.load(input_file)\n","JSON_IDX = 1\n","params = json_array[JSON_IDX]\n","\n","EMBEDDING_DIM = params[\"EMBEDDING_DIM\"]\n","HIDDEN_DIM = params[\"HIDDEN_DIM\"]\n","LEARNING_RATE = params[\"LEARNING_RATE\"]\n","WEIGHT_DECAY = params[\"WEIGHT_DECAY\"]\n","EPOCHS = params[\"EPOCHS\"]\n","MOMENTUM = params[\"MOMENTUM\"]\n","BATCH_SIZE = params[\"BATCH_SIZE\"]\n","\n","#HArd coding for now\n","BATCH_SIZE = 16\n","LEARNING_RATE = 1e-4\n","EMBEDDING_DIM = 1024\n","\n","print(params)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'EMBEDDING_DIM': 64, 'HIDDEN_DIM': 256, 'LEARNING_RATE': 0.1, 'WEIGHT_DECAY': 0.0, 'EPOCHS': 100, 'MOMENTUM': 0.9, 'BATCH_SIZE': 32}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xYf4_7KjosZA","colab_type":"code","colab":{}},"source":["train_loader = DataLoader(punc_dataset,BATCH_SIZE, kwargs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYn_YbfzosZG","colab_type":"code","colab":{}},"source":["class LSTMTagger(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","        # The LSTM takes word embeddings as inputs, and outputs hidden states\n","        # with dimensionality hidden_dim.\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, tagset_size, bidirectional=True, batch_first=True)\n","        self.lstm2 = nn.LSTM(hidden_dim*2, hidden_dim, tagset_size, bidirectional=True, batch_first=True)\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","#         print(embeds.shape)\n","#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n","        lstm_1,_ = self.lstm(embeds)\n","        lstm_out,_ = self.lstm2(lstm_1)\n","#         print(lstm_out.shape)\n","#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n","        tag_space = self.hidden2tag(lstm_out)\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fnEri1aosZK","colab_type":"code","colab":{}},"source":["def save_model(epoch):\n","    path = \"drive/My Drive/Colab Notebooks/models/\" + CURR_NOTEBOOK + \"/\"\n","    model_name = \"model_\" + str(JSON_IDX) + \"_\" + time.strftime(\"%m%d%Y_%H%M\") + \"_\" + str(epoch)\n","    torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': running_loss,\n","                }, path + model_name)\n","    print(model_name, \"saved.\")\n","    return model_name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AEvq6iwosZP","colab_type":"code","colab":{}},"source":["def get_val_accuracy(verbose=False):\n","    # See what the scores are after training\n","    scores = []\n","    true_pos = {0:0, 1:0, 2:0, 3:0}\n","    false_neg = {0:0,1:0,2:0, 3:0}\n","    false_pos = {0:0,1:0,2:0,3:0}\n","    step = 1000   # How many instances to process at once. Should fix memory issue\n","    with torch.no_grad():\n","        for k in range(0, len(punc_dataset.val_data)-step, step):\n","            inputs = punc_dataset.val_data[k:k+step]\n","            outputs = punc_dataset.val_labels[k:k+step]\n","            if cuda:\n","                inputs = inputs.cuda()\n","                outputs = outputs.cuda()\n","\n","            tag_scores = model(inputs)\n","            scores = torch.argmax(tag_scores, axis=2).cpu().numpy()\n","\n","\n","            for i in range(len(scores)):\n","                for j in range(len(scores[i])):\n","                    op = outputs[i][j].item()\n","                    if op == scores[i][j]:\n","                        true_pos[op] += 1\n","                    else:\n","                        false_pos[scores[i][j]] += 1\n","                        false_neg[op] += 1\n","    if verbose:\n","        print(true_pos)\n","        print(false_pos)\n","        print(false_neg)\n","    \n","    for i in range(1, len(punc_dataset.tag_to_ix)):\n","        pre = true_pos[i] / (true_pos[i] + false_pos[i])\n","#         print(\"Precision of \",i,\": \", pre)\n","        rec = true_pos[i] / (true_pos[i] + false_neg[i])\n","#         print(\"Recall of \" , i , \": \", rec)\n","        f_score = 2 * pre * rec / (pre + rec)\n","#         print(\"F score of \" , i , \": \" , f_score)\n","        if verbose:\n","            print(\"Precision, Recall, F score of\", i, \": \", pre, rec, f_score)\n","        \n","    total_true_pos = 0\n","    total_false_pos = 0\n","    total_false_neg = 0\n","\n","    for i in range(1, len(punc_dataset.tag_to_ix)):\n","        total_true_pos += true_pos[i]\n","        total_false_pos += false_pos[i]\n","        total_false_neg += false_neg[i]\n","\n","    pre = total_true_pos / (total_true_pos + total_false_pos)\n","#     print(\"Total precision: \", pre)\n","    rec = total_true_pos / (total_true_pos + total_false_neg)\n","#     print(\"Total recall: \", rec)\n","    f_score = 2 * pre * rec / (pre + rec)\n","#     print(\"Total F1 score \", f_score)\n","    print(\"Total Precision, Recall, F score: \", pre, rec, f_score)\n","    return f_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBcB3E8UhF3U","colab_type":"code","colab":{}},"source":["def get_test_accuracy(verbose=False):\n","    # See what the scores are after training\n","    scores = []\n","    true_pos = {0:0, 1:0, 2:0, 3:0}\n","    false_neg = {0:0,1:0,2:0, 3:0}\n","    false_pos = {0:0,1:0,2:0,3:0}\n","    step = 1000   # How many instances to process at once. Should fix memory issue\n","    with torch.no_grad():\n","        for k in range(0, len(punc_dataset.test_data)-step, step):\n","            inputs = punc_dataset.test_data[k:k+step]\n","            outputs = punc_dataset.test_labels[k:k+step]\n","            if cuda:\n","                inputs = inputs.cuda()\n","                outputs = outputs.cuda()\n","\n","            tag_scores = model(inputs)\n","            scores = torch.argmax(tag_scores, axis=2).cpu().numpy()\n","\n","\n","            for i in range(len(scores)):\n","                for j in range(len(scores[i])):\n","                    op = outputs[i][j].item()\n","                    if op == scores[i][j]:\n","                        true_pos[op] += 1\n","                    else:\n","                        false_pos[scores[i][j]] += 1\n","                        false_neg[op] += 1\n","    if verbose:\n","        print(true_pos)\n","        print(false_pos)\n","        print(false_neg)\n","    \n","    for i in range(1, len(punc_dataset.tag_to_ix)):\n","        pre = true_pos[i] / (true_pos[i] + false_pos[i])\n","#         print(\"Precision of \",i,\": \", pre)\n","        rec = true_pos[i] / (true_pos[i] + false_neg[i])\n","#         print(\"Recall of \" , i , \": \", rec)\n","        f_score = 2 * pre * rec / (pre + rec)\n","#         print(\"F score of \" , i , \": \" , f_score)\n","        if verbose:\n","            print(\"TEST Precision, Recall, F score of\", i, \": \", pre, rec, f_score)\n","        \n","    total_true_pos = 0\n","    total_false_pos = 0\n","    total_false_neg = 0\n","\n","    for i in range(1, len(punc_dataset.tag_to_ix)):\n","        total_true_pos += true_pos[i]\n","        total_false_pos += false_pos[i]\n","        total_false_neg += false_neg[i]\n","\n","    pre = total_true_pos / (total_true_pos + total_false_pos)\n","#     print(\"Total precision: \", pre)\n","    rec = total_true_pos / (total_true_pos + total_false_neg)\n","#     print(\"Total recall: \", rec)\n","    f_score = 2 * pre * rec / (pre + rec)\n","#     print(\"Total F1 score \", f_score)\n","    print(\"TEST Total Precision, Recall, F score: \", pre, rec, f_score)\n","    return f_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-LZOfoV5XeQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ca1b2c6d-e9e1-4259-ab56-b01d2f6408a4","executionInfo":{"status":"ok","timestamp":1587690351043,"user_tz":240,"elapsed":32468,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}}},"source":["load = True\n","\n","model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(punc_dataset.word_to_ix), len(punc_dataset.tag_to_ix))\n","if cuda:\n","  model.cuda()\n","loss_function = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) #, momentum=MOMENTUM)\n","\n","start_epoch = 1\n","\n","if load:\n","  #LOAD_MODEL\n","  model_name = \"model_1_04232020_1705_50\"\n","  # Hard coding learning rate decay\n","  # WEIGHT_DECAY = 0.99\n","  model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(punc_dataset.word_to_ix), len(punc_dataset.tag_to_ix))\n","  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # momentum=MOMENTUM)\n","  if cuda:\n","    model.cuda()\n","  path = \"drive/My Drive/Colab Notebooks\"\n","  checkpoint = torch.load(path + \"/models/\" + CURR_NOTEBOOK + \"/\" + model_name)\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  start_epoch = checkpoint['epoch']\n","  loss = checkpoint['loss']\n","  print(\"Epoch from loaded model: \", start_epoch)\n","  print(\"Loss from loaded model: \", loss.item())\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch from loaded model:  50\n","Loss from loaded model:  9264.298828125\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X50jn11qosZR","colab_type":"code","outputId":"2c4c2919-14cf-4f51-a121-02ae7e14badd","executionInfo":{"status":"ok","timestamp":1587512587809,"user_tz":240,"elapsed":10111143,"user":{"displayName":"Sumedha Raman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgysisVcE5JN5U2v6caaMxXPUCPWWbIMrnHoAx4Kw=s64","userId":"13191497311495899179"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["running_loss = 0.0\n","prev_f1 = 0.0\n","SAVE_FREQ = 10\n","print(\"Start Training! \")\n","start_time = time.time()\n","for epoch in range(start_epoch, EPOCHS + start_epoch + 1):\n","    running_loss = 0.0\n","    for batch_idx, batch in enumerate(train_loader):\n","        model.zero_grad()\n","        sentence_in, targets = Variable(batch[0]), Variable(batch[1])\n","        targets = targets.view(-1)\n","        \n","        if cuda:\n","            sentence_in = sentence_in.cuda()\n","            targets = targets.cuda()\n","            \n","        tag_scores = model(sentence_in)\n","        tag_scores = tag_scores.view(-1, 4)\n","\n","        loss = loss_function(tag_scores, targets)\n","        running_loss += loss\n","        loss.backward()\n","        optimizer.step()\n","    print(\"Epoch: \", epoch, \"Loss: \", running_loss.item())\n","    # print(\"F1 Score:\", get_val_accuracy())\n","    if epoch % SAVE_FREQ == 0:\n","        curr_f1 = get_val_accuracy()\n","        print(\"F1 Score\", curr_f1)\n","        if curr_f1 > prev_f1:\n","            prev_f1 = curr_f1\n","            save_model(epoch)\n","            \n","print(\"Run time: \",(time.time() - start_time)/60, \"minutes\")        \n","get_val_accuracy(True)\n","model_name = save_model(EPOCHS+start_epoch)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Start Training! \n","Epoch:  30 Loss:  9271.6875\n","Total Precision, Recall, F score:  0.49718050604803365 0.5505443486419593 0.5225034351994718\n","F1 Score 0.5225034351994718\n","model_1_04232020_1500_30 saved.\n","Epoch:  31 Loss:  9270.6904296875\n","Epoch:  32 Loss:  9270.16796875\n","Epoch:  33 Loss:  9269.39453125\n","Epoch:  34 Loss:  9269.310546875\n","Epoch:  35 Loss:  9268.197265625\n","Epoch:  36 Loss:  9268.271484375\n","Epoch:  37 Loss:  9268.107421875\n","Epoch:  38 Loss:  9267.693359375\n","Epoch:  39 Loss:  9266.77734375\n","Epoch:  40 Loss:  9266.7763671875\n","Total Precision, Recall, F score:  0.48694969215018025 0.5616577963990488 0.5216424525608858\n","F1 Score 0.5216424525608858\n","Epoch:  41 Loss:  9266.255859375\n","Epoch:  42 Loss:  9266.125\n","Epoch:  43 Loss:  9265.7158203125\n","Epoch:  44 Loss:  9265.625\n","Epoch:  45 Loss:  9265.4794921875\n","Epoch:  46 Loss:  9264.796875\n","Epoch:  47 Loss:  9264.912109375\n","Epoch:  48 Loss:  9264.6689453125\n","Epoch:  49 Loss:  9264.310546875\n","Epoch:  50 Loss:  9264.298828125\n","Total Precision, Recall, F score:  0.5092674580900782 0.548926670656939 0.5283538863976083\n","F1 Score 0.5283538863976083\n","model_1_04232020_1705_50 saved.\n","Epoch:  51 Loss:  9264.3896484375\n","Epoch:  52 Loss:  9264.0947265625\n","Epoch:  53 Loss:  9263.822265625\n","Epoch:  54 Loss:  9263.8984375\n","Epoch:  55 Loss:  9263.380859375\n","Epoch:  56 Loss:  9263.296875\n","Epoch:  57 Loss:  9263.197265625\n","Epoch:  58 Loss:  9263.1904296875\n","Epoch:  59 Loss:  9263.158203125\n","Epoch:  60 Loss:  9262.8046875\n","Total Precision, Recall, F score:  0.4878885835125244 0.5542326544478057 0.5189488033929114\n","F1 Score 0.5189488033929114\n","Epoch:  61 Loss:  9262.8701171875\n","Epoch:  62 Loss:  9262.427734375\n","Epoch:  63 Loss:  9262.5703125\n","Epoch:  64 Loss:  9262.5615234375\n","Epoch:  65 Loss:  9262.2900390625\n","Epoch:  66 Loss:  9262.78515625\n","Epoch:  67 Loss:  9262.4794921875\n","Epoch:  68 Loss:  9262.4404296875\n","Epoch:  69 Loss:  9262.279296875\n","Epoch:  70 Loss:  9262.3974609375\n","Total Precision, Recall, F score:  0.49931112131672123 0.5452221880712426 0.5212576748789804\n","F1 Score 0.5212576748789804\n","Epoch:  71 Loss:  9262.20703125\n","Epoch:  72 Loss:  9262.078125\n","Epoch:  73 Loss:  9261.8955078125\n","Epoch:  74 Loss:  9261.7021484375\n","Epoch:  75 Loss:  9261.818359375\n","Epoch:  76 Loss:  9261.6962890625\n","Epoch:  77 Loss:  9261.6015625\n","Epoch:  78 Loss:  9261.990234375\n","Epoch:  79 Loss:  9261.3828125\n","Epoch:  80 Loss:  9261.5869140625\n","Total Precision, Recall, F score:  0.5022267387990555 0.5436368636459227 0.5221120009943216\n","F1 Score 0.5221120009943216\n","Epoch:  81 Loss:  9261.703125\n","Epoch:  82 Loss:  9261.8642578125\n","Epoch:  83 Loss:  9261.33984375\n","Epoch:  84 Loss:  9261.298828125\n","Epoch:  85 Loss:  9261.41796875\n","Epoch:  86 Loss:  9260.9619140625\n","Epoch:  87 Loss:  9260.984375\n","Epoch:  88 Loss:  9261.51171875\n","Epoch:  89 Loss:  9261.11328125\n","Epoch:  90 Loss:  9260.9296875\n","Total Precision, Recall, F score:  0.49816619664801204 0.5515149554329715 0.5234848833817004\n","F1 Score 0.5234848833817004\n","Epoch:  91 Loss:  9260.7890625\n","Epoch:  92 Loss:  9260.9775390625\n","Epoch:  93 Loss:  9260.966796875\n","Epoch:  94 Loss:  9260.9580078125\n","Epoch:  95 Loss:  9260.4775390625\n","Epoch:  96 Loss:  9260.720703125\n","Epoch:  97 Loss:  9260.7216796875\n","Epoch:  98 Loss:  9260.29296875\n","Epoch:  99 Loss:  9260.673828125\n","Epoch:  100 Loss:  9260.521484375\n","Total Precision, Recall, F score:  0.49916264981287045 0.5544914829254088 0.5253743696641785\n","F1 Score 0.5253743696641785\n","Epoch:  101 Loss:  9260.2958984375\n","Epoch:  102 Loss:  9260.3115234375\n","Epoch:  103 Loss:  9260.37890625\n","Epoch:  104 Loss:  9260.58203125\n","Epoch:  105 Loss:  9260.3369140625\n","Epoch:  106 Loss:  9260.1220703125\n","Epoch:  107 Loss:  9260.306640625\n","Epoch:  108 Loss:  9260.2529296875\n","Epoch:  109 Loss:  9260.1982421875\n","Epoch:  110 Loss:  9260.056640625\n","Total Precision, Recall, F score:  0.48609016617116924 0.5475031140301212 0.514972155442622\n","F1 Score 0.514972155442622\n","Epoch:  111 Loss:  9260.162109375\n","Epoch:  112 Loss:  9260.142578125\n","Epoch:  113 Loss:  9260.291015625\n","Epoch:  114 Loss:  9259.7041015625\n","Epoch:  115 Loss:  9259.896484375\n","Epoch:  116 Loss:  9260.4814453125\n","Epoch:  117 Loss:  9259.8203125\n","Epoch:  118 Loss:  9259.830078125\n","Epoch:  119 Loss:  9259.478515625\n","Epoch:  120 Loss:  9259.5771484375\n","Total Precision, Recall, F score:  0.5027505561233692 0.5410971092094408 0.5212194874911374\n","F1 Score 0.5212194874911374\n","Epoch:  121 Loss:  9259.5859375\n","Epoch:  122 Loss:  9259.880859375\n","Epoch:  123 Loss:  9259.6953125\n","Epoch:  124 Loss:  9259.7568359375\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VZHmZ9T1osZZ","colab_type":"code","colab":{}},"source":["# load = True\n","\n","# model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(punc_dataset.word_to_ix), len(punc_dataset.tag_to_ix))\n","# if cuda:\n","#   model.cuda()\n","# loss_function = nn.NLLLoss()\n","# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) #, momentum=MOMENTUM)\n","\n","# start_epoch = 1\n","\n","# if load:\n","#   #LOAD_MODEL\n","#   model_name = \"model_1_04192020_2322_80\"\n","#   # Hard coding learning rate decay\n","#   # WEIGHT_DECAY = 0.99\n","#   model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(punc_dataset.word_to_ix), len(punc_dataset.tag_to_ix))\n","#   optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # momentum=MOMENTUM)\n","#   if cuda:\n","#     model.cuda()\n","#   path = \"drive/My Drive/Colab Notebooks\"\n","#   checkpoint = torch.load(path + \"/models/\" + CURR_NOTEBOOK + \"/\" + model_name)\n","#   model.load_state_dict(checkpoint['model_state_dict'])\n","#   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","#   start_epoch = checkpoint['epoch']\n","#   loss = checkpoint['loss']\n","#   print(\"Epoch from loaded model: \", start_epoch)\n","#   print(\"Loss from loaded model: \", loss.item())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4vi9pD-CCbs","colab_type":"code","outputId":"dda0fca2-433f-462a-a8a7-9f9748adabd3","executionInfo":{"status":"ok","timestamp":1587690370795,"user_tz":240,"elapsed":7676,"user":{"displayName":"Cesar S","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjthpy5DotgzR8OgIbD2BZ9AGERttcDefZx6FSmvQ=s64","userId":"10672901347282825724"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["get_test_accuracy(True)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["{0: 184762, 1: 7117, 2: 8978, 3: 639}\n","{0: 5802, 1: 3474, 2: 6606, 3: 6622}\n","{0: 8337, 1: 6274, 2: 7482, 3: 411}\n","TEST Precision, Recall, F score of 1 :  0.671985648191861 0.5314763647225749 0.5935284796931033\n","TEST Precision, Recall, F score of 2 :  0.5761036960985626 0.5454434993924666 0.5603545125452503\n","TEST Precision, Recall, F score of 3 :  0.08800440710645917 0.6085714285714285 0.1537721092527975\n","TEST Total Precision, Recall, F score:  0.5004785261394904 0.5415358726254814 0.5201983306650918\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.5201983306650918"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"0xAct3WAlosl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}